# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax
import jax.numpy as jnp
from jax import grad
from metrics import *
from sklearn.linear_model import LinearRegression as SKLR
from mpl_toolkits.mplot3d import Axes3D

from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import matplotlib.lines as lines
import os
np.random.seed(45)

class LinearRegression():
  def __init__(self, fit_intercept = True):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.intercept_ = 0
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=[] # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    self.X_train=None
    self.y_train=None
    self.number_params=None
  

  def fit_sklearn_LR(self, X, y):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters

    self.X_train=X
    self.y_train=y

    reg = SKLR(fit_intercept=self.fit_intercept).fit(X, y)
    self.coef_ = reg.coef_
    self.intercept_ = reg.intercept_

  def fit_normal_equations(self, X, y):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    self.X_train=X
    self.y_train=y

    X_=X.values
    if(self.fit_intercept==True):
      ones_col = np.ones((X_.shape[0]))
      X_ = np.insert(X_, 0, ones_col, axis=1)
    
    x_transpose=X_.copy().transpose()
    mat1=np.matmul(x_transpose,X_)
    mat1=np.linalg.inv(mat1)
    final=np.matmul(mat1, np.matmul(x_transpose,y))

    if(self.fit_intercept):
      self.intercept_=final[0]
      self.coef_=final[1:]
    else:
      self.coef_=final

  def fit_SVD(self,X,y):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix
    self.X_train=X
    self.y_train=y

    X_=X.values

    if(self.fit_intercept==True):
      ones_col = np.ones((X_.shape[0]))
      X_ = np.insert(X_, 0, ones_col, axis=1)
    U,S,Vt = np.linalg.svd(X_, full_matrices=False)
    final = Vt.T @ np.linalg.inv(np.diag(S)) @ U.T @ y
    
    if(self.fit_intercept):
      self.intercept_=final[0]
      self.coef_=final[1:]
    else:
      self.coef_=final
    
  def mse_loss(self,X,y):                
    # Compute the MSE loss with the learned model
    y_hat=self.predict(X)
    rmse_model=rmse(y_hat, y)
    mse=rmse_model**2
    return mse

  def compute_gradient(self,X,y, penalty=None):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    
    y_hat=self.predict(X)
    X=X.values
    ones_col = np.ones((X.shape[0]))
    X = np.insert(X, 0, ones_col, axis=1)

    epsilon=y-y_hat

    gradients_=np.zeros(self.coef_.shape[0]+1)## first index is the gradient of the bias

    if(penalty=='l2'):
      for i in range(len(gradients_)):
        gradients_[i]=-2*np.mean(epsilon*X[:,i])

      mu=0.05
      gradients_[0]+=mu*2*self.intercept_
      for i in range(len(self.coef_)):
        gradients_[i+1]+=mu*2*self.coef_[i]

    else:
      for i in range(len(gradients_)):
        gradients_[i]=-2*np.mean(epsilon*X[:,i])

    return gradients_

  def compute_jax_gradient(self,X_,y_,penalty=None):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
    
    X=X_#e example, polyfit from numpy has no problem finding the model. You can see the plot and the code below.


    y=y_
      
    all_params=np.zeros(X.shape[1]+1)
    all_params[0]=self.intercept_
    all_params[1:]=  self.coef_

    all_params=jnp.array(all_params)
    X=jnp.array(X)
    y=jnp.array(y)

    def mse_loss(y, y_pred):
      return jnp.mean(jnp.square(y - y_pred))

    def loss_fn(all_params,X,y):
      mult = X * all_params[1:]
      sum_ = mult.sum(axis=1)
      y_pred = sum_ + all_params[0]
      result=mse_loss(y, y_pred)
      return result

    #print(loss_fn(all_params, X, y),"$$$$$$$$$$$ LOSSSSSSSSS")
    def ridge(mu,all_params):
      result=mu*jnp.sum(jnp.square(all_params))
      return result

    def lasso(mu,all_params):
      result=mu*jnp.sum(jnp.absolute(all_params))
      return result

    if(penalty==None):
      grad_fn=grad(loss_fn)
      grads=grad_fn(all_params,X,y)

    elif(penalty=='l2'):

      grad_fn=grad(loss_fn)
      grads=grad_fn(all_params,X,y)
      
      ridge_fn=grad(ridge,argnums=1)
      ridge_grad=ridge_fn(0.05,all_params)
      grads+=ridge_grad

    else:

      grad_fn=grad(loss_fn)
      grads=grad_fn(all_params,X,y)
      
      lasso_fn=grad(lasso,argnums=1)
      lasso_grad=lasso_fn(0.05,all_params)
      grads+=lasso_grad
    
    return grads

  def fit_gradient_descent(self, X_train,y_train,batch_size, gradient_type, penalty_type, num_iters=20, lr=0.01):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
    X=X_train
    y=y_train 
    self.intercept_=0
    self.coef_=np.zeros(X.shape[1])
    
    if(gradient_type=='jax'):
      all_params=np.zeros(X.shape[1]+1)
      self.all_coef.append(all_params)
      all_params=jnp.array(all_params)
      X=jnp.array(X.values)
      y=jnp.array(y)
      for i in range(num_iters):
        X_,y_=shuffle(X,y)
        num_batches=len(X_)//batch_size
        for j in range(num_batches):
          X_batch=X_[j*batch_size:(j+1)*batch_size]
          y_batch=y_[j*batch_size:(j+1)*batch_size]

          grads=self.compute_jax_gradient(X_batch,y_batch,penalty=penalty_type)
          if(self.fit_intercept == True):
            grads=grads.at[0].set(0)
          all_params-=lr*grads
          self.intercept_=all_params[0]
          self.coef_=all_params[1:]
        self.all_coef.append(all_params)

      self.all_coef=np.array(self.all_coef)
      
    else:
      all_params=np.zeros(X.shape[1]+1)
      self.all_coef.append(all_params)
      y=y

      for i in range(num_iters):
        X_,y_=shuffle(X,y)
        num_batches=len(X_)//batch_size
        for j in range(num_batches):
          X_batch=X_[j*batch_size:(j+1)*batch_size]
          y_batch=y_[j*batch_size:(j+1)*batch_size]

          grads=self.compute_gradient(X_batch,y_batch,penalty=penalty_type)
          if(self.fit_intercept == True):
            grads[0]=0
          all_params-=lr*grads
          self.intercept_=all_params[0]
          self.coef_=all_params[1:]

        self.all_coef.append(all_params)
        
      self.all_coef=np.array(self.all_coef)
      self.intercept_=all_params[0]
      self.coef_=all_params[1:]

  def fit_SGD_with_momentum(self,X,y,num_iters,lr,penalty='l2', beta=0.9):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
      self.intercept_=0
      self.coef_=np.zeros(X.shape[1])
      
      all_params=np.zeros(X.shape[1]+1)
      self.all_coef.append(all_params)

      y=y
      param_t1=None
      param_t0=None
      for i in range(num_iters):
        X_,y_=shuffle(X,y)
        batch_size=1
        num_batches=len(X_)//batch_size
        for j in range(num_batches):
          X_batch=X_[j*batch_size:(j+1)*batch_size]
          y_batch=y_[j*batch_size:(j+1)*batch_size]
          grads=self.compute_gradient(X_batch,y_batch,penalty=penalty)
          if(self.fit_intercept == True):
            grads[0]=0
          if(i==0 and j==0):
            param_t0=all_params
            all_params-=lr*grads
          elif(i==0 and j==1):
            param_t1=all_params
            all_params-=lr*grads
          else:
            #print(param_t0, param_t1, i, j, '##########3')
            all_params=-lr*grads+beta*(param_t1-param_t0)
            param_t0=param_t1
            param_t1=all_params
          self.all_coef.append(all_params)
            
      self.intercept_=all_params[0]
      self.coef_=all_params[1:]

  def predict(self, X):
    # Funtion to run the LinearRegression on a test data point
    arr_X=X.values
    mult=arr_X*self.coef_
    sum_=mult.sum(axis=1)
    answer=sum_+self.intercept_
    return answer

  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    theta_0_vals=np.linspace(self.all_coef[0].min()-3,self.all_coef[0].max()+3,100)
    theta_1_vals=np.linspace(self.all_coef[1].min()-3,self.all_coef[1].max()+3,100)

    theta_0_vals,theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)

    r1, r2 = theta_0_vals.flatten(), theta_1_vals.flatten()
    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))

    grid = np.hstack((r1,r2))
    mse_arr=[]
    for i in range(len(grid)):
      theta_0_=grid[i][0]
      theta_1_=grid[i][1]
      y_hat=theta_1_*X+theta_0_
      mse=np.mean(np.square(y - y_hat))
      mse_arr.append(mse)
    mse_arr=np.array(mse_arr)
    mse_arr=mse_arr.reshape((theta_0_vals.shape[0],theta_1_vals.shape[0]))
    
    fig = plt.figure()
    ax1 = fig.gca(projection='3d')

    temp = ax1.plot_surface(theta_0_vals, theta_1_vals, mse_arr, color='paleturquoise', alpha=0.5)
    ax1.view_init(20, -240)
    mse_iter=[]
    for i in range(len(self.all_coef)):
      theta_0_=self.all_coef[i][0]
      theta_1_=self.all_coef[i][1]
      y_hat=theta_1_*X+theta_0_
      mse=np.mean(np.square(y - y_hat))
      mse_iter.append(mse)

      ax1.scatter(theta_0_,theta_1_,mse,color='red')
      file_name=str(i)+'.png'
      path=os.path.join('/home/divyanshu/Documents/Academics/Sem8/Machine Learning/Assignment4/surface_plots', file_name)
      plt.savefig(path)

  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    for i in range( len(theta_0)):
      fig = plt.figure()
      plt.scatter(X,y)
      y_hat=theta_1[i]*X+theta_0[i]
      plt.plot(X,y_hat, color='red')
      plt.xlabel('X')
      plt.ylabel('Y')
      plt.title('Scatter Plot of Points')
      file_name=str(i)+'.png'
      path=os.path.join('/home/divyanshu/Documents/Academics/Sem8/Machine Learning/Assignment4/line_fit_figures', file_name)
      plt.savefig(path)

  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    theta_0_vals=np.linspace(self.all_coef[0].min()-3,self.all_coef[0].max()+10,100)
    theta_1_vals=np.linspace(self.all_coef[1].min()-3,self.all_coef[1].max()+10,100)

    theta_0_vals,theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)

    r1, r2 = theta_0_vals.flatten(), theta_1_vals.flatten()
    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))

    grid = np.hstack((r1,r2))
    mse_arr=[]
    for i in range(len(grid)):
      theta_0_=grid[i][0]
      theta_1_=grid[i][1]
      y_hat=theta_1_*X+theta_0_
      mse=np.mean(np.square(y - y_hat))
      mse_arr.append(mse)
    mse_arr=np.array(mse_arr)
    mse_arr=mse_arr.reshape((theta_0_vals.shape[0],theta_1_vals.shape[0]))
    
    fig, ax1 = plt.subplots(1, 1)
    
    levels = np.linspace(int(np.min(mse_arr))-0.5, int(np.max(mse_arr))+0.5, 20)
    contours = ax1.contour(theta_0_vals, theta_1_vals, mse_arr, levels=levels, cmap='spring',alpha=0.5)
    ax1.clabel(contours, inline=True, fontsize=8)

    mse_iter=[]
    for i in range(len(self.all_coef)):
      theta_0_=self.all_coef[i][0]
      theta_1_=self.all_coef[i][1]
      y_hat=theta_1_*X+theta_0_
      mse=np.mean(np.square(y - y_hat))
      mse_iter.append(mse)

      ax1.scatter(theta_0_,theta_1_,color='red')
      file_name=str(i)+'.png'
      path=os.path.join('/home/divyanshu/Documents/Academics/Sem8/Machine Learning/Assignment4/contour_plots', file_name)
      plt.savefig(path)